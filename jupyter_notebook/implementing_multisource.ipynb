{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Source : CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_excel(r'C:\\Users\\Samruddhi More\\Desktop\\LLM BE Template\\data\\sample1.xlsx')\n",
    "\n",
    "df.to_csv('sample1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import CSVLoader\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import OpenAI\n",
    "import os\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import openai\n",
    "openai.api_key = os.environ.get(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Samruddhi More\\Desktop\\LLM BE Template\\venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "# Initialize the OpenAI embeddings\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=openai.api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the documents\n",
    "loader = CSVLoader(file_path=r'C:\\Users\\Samruddhi More\\Desktop\\LLM BE Template\\data\\1 to Many.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Samruddhi More\\Desktop\\LLM BE Template\\venv\\Lib\\site-packages\\langchain\\indexes\\vectorstore.py:129: UserWarning: Using InMemoryVectorStore as the default vectorstore.This memory store won't persist data. You should explicitlyspecify a vectorstore when using VectorstoreIndexCreator\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Create an index using the loaded documents\n",
    "index_creator = VectorstoreIndexCreator(embedding=embeddings)\n",
    "docsearch = index_creator.from_loaders([loader])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Samruddhi More\\Desktop\\LLM BE Template\\venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The class `OpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "# Create a question-answering chain using the index\n",
    "llm = OpenAI(model_name=\"gpt-3.5-turbo-instruct\", openai_api_key=openai.api_key)\n",
    "chain = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=docsearch.vectorstore.as_retriever(), input_key=\"question\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Samruddhi More\\Desktop\\LLM BE Template\\venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "# Pass a query to the chain\n",
    "query = \"Summarize the doument\"\n",
    "response = chain({\"question\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(type(response['result']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Source: PDF doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import PyPDF2\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "# Importing necessary modules from langchain and langchain_openai\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.callbacks import get_openai_callback\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import openai\n",
    "openai.api_key = os.environ.get(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdfpath = r'C:\\Users\\Samruddhi More\\Desktop\\LLM BE Template\\data\\Vedanta Cairn_May 2024_PO No.7300175294 (1).PDF'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_processing(pdf):\n",
    "    \"\"\"\n",
    "    Process a PDF file, extracting text and splitting it into chunks.\n",
    "    \"\"\"\n",
    "    pdf_reader = PdfReader(pdf)\n",
    "        \n",
    "    text = \"\"\n",
    "    for page in pdf_reader.pages:\n",
    "        text += page.extract_text()\n",
    "        \n",
    "    # Split text into chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=200,\n",
    "        length_function=len\n",
    "    )\n",
    "    chunks = text_splitter.split_text(text=text)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embeddings(chunks):\n",
    "    \"\"\"\n",
    "    Create embeddings for text chunks using OpenAI.\n",
    "    \"\"\"\n",
    "    # Initialize OpenAI embeddings\n",
    "    embeddings = OpenAIEmbeddings(openai_api_key=openai.api_key)\n",
    "    # Create vector store using FAISS\n",
    "    vector_store = FAISS.from_texts(chunks, embedding=embeddings)\n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generation(VectorStore):\n",
    "    \"\"\"\n",
    "    Generate responses based on prompts and embeddings.\n",
    "    \"\"\"\n",
    "    retriever = VectorStore.as_retriever()\n",
    "    \n",
    "    # Define template for prompts\n",
    "    template = \"\"\"Respond to the prompt based on the following context: {context}\n",
    "    Questions: {questions}\n",
    "    \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "    # Initialize ChatOpenAI model\n",
    "    model = ChatOpenAI(openai_api_key=openai.api_key)\n",
    "    \n",
    "    # Define processing chain\n",
    "    chain = (\n",
    "        {\"context\": retriever, \"questions\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | model\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    \n",
    "    # Prompt user for input\n",
    "    query = input(\"Insert Prompt: \")\n",
    "    \n",
    "    # Invoke the processing chain with user input\n",
    "    output = chain.invoke(query)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(generation(embeddings(chunk_processing(pdfpath))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Source: Docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Samruddhi More\\Desktop\\LLM BE Template\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#Convert the docx to pdf\n",
    "from docx2pdf import convert\n",
    "\n",
    "def convert_docx_to_pdf(input_path, output_path):\n",
    "    try:\n",
    "        # Convert DOCX to PDF\n",
    "        convert(input_path, output_path)\n",
    "        print(f\"Conversion complete. PDF saved at {output_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:03<00:00,  3.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion complete. PDF saved at document_0.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "convert_docx_to_pdf(r'C:\\Users\\Samruddhi More\\Desktop\\LLM BE Template\\data\\document_0.docx', 'document_0.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df =pd.read_csv(r'C:\\Users\\Samruddhi More\\Desktop\\LLM BE Template\\data\\1 to Many.csv')\n",
    "df.to_excel('1 to Many.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a standarad Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Samruddhi More\\Desktop\\LLM BE Template\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import CSVLoader\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import OpenAI\n",
    "import os\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "import PyPDF2\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "# Importing necessary modules from langchain and langchain_openai\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.callbacks import get_openai_callback\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI\n",
    "#Convert the docx to pdf\n",
    "from docx2pdf import convert\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import openai\n",
    "openai.api_key = os.environ.get(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataTalk:\n",
    "\n",
    "    def __init__(self, filepath):\n",
    "        self.filepath = filepath\n",
    "\n",
    "    def source_type(self):\n",
    "        file = ''\n",
    "        folderpath = os.path.dirname(self.filepath)\n",
    "        filename = os.path.basename(self.filepath)\n",
    "        if filename.endswith('.xlsx'):\n",
    "            csvname = filename.split('.')[0] + '.csv'\n",
    "            file = os.path.join(folderpath, csvname)\n",
    "            df = pd.read_excel(self.filepath)\n",
    "            df.to_csv(file)\n",
    "        elif filename.endswith('.docx'):\n",
    "            pdfname = filename.split('.')[0] + '.pdf'\n",
    "            file = os.path.join(folderpath, pdfname)\n",
    "            convert(self.filepath, file)\n",
    "        else: \n",
    "            file = self.filepath\n",
    "\n",
    "        return file\n",
    "    \n",
    "    \n",
    "    def generate_response(self, query):\n",
    "        # Initialize the OpenAI embeddings\n",
    "        embeddings = OpenAIEmbeddings(openai_api_key=openai.api_key)\n",
    "        model_name=\"gpt-3.5-turbo-instruct\"\n",
    "        file = self.source_type()\n",
    "        response = ''\n",
    "\n",
    "        if file.endswith('.csv'):\n",
    "            # Load the documents\n",
    "            loader = CSVLoader(file_path=file)\n",
    "            # Create an index using the loaded documents\n",
    "            index_creator = VectorstoreIndexCreator(embedding=embeddings)\n",
    "            docsearch = index_creator.from_loaders([loader])\n",
    "            # Create a question-answering chain using the index\n",
    "            llm = OpenAI(model_name=model_name, openai_api_key=openai.api_key)\n",
    "            chain = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=docsearch.vectorstore.as_retriever(), input_key=\"question\")\n",
    "            response = chain({\"question\": query})\n",
    "            response = response['result']\n",
    "        \n",
    "        elif file.lower().endswith('.pdf'):\n",
    "\n",
    "            \"\"\"\n",
    "            Process a PDF file, extracting text and splitting it into chunks.\n",
    "            \"\"\"\n",
    "            pdf_reader = PdfReader(file)\n",
    "                \n",
    "            text = \"\"\n",
    "            for page in pdf_reader.pages:\n",
    "                text += page.extract_text()\n",
    "                \n",
    "            # Split text into chunks\n",
    "            text_splitter = RecursiveCharacterTextSplitter(\n",
    "                chunk_size=1000,\n",
    "                chunk_overlap=200,\n",
    "                length_function=len\n",
    "            )\n",
    "            chunks = text_splitter.split_text(text=text)\n",
    "\n",
    "            # Create vector store using FAISS\n",
    "            vector_store = FAISS.from_texts(chunks, embedding=embeddings)\n",
    "\n",
    "            \"\"\"\n",
    "            Generate responses based on prompts and embeddings.\n",
    "            \"\"\"\n",
    "            retriever = vector_store.as_retriever()\n",
    "            \n",
    "            # Define template for prompts\n",
    "            template = \"\"\"Respond to the prompt based on the following context: {context}\n",
    "            Questions: {questions}\n",
    "            \"\"\"\n",
    "            prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "            # Initialize ChatOpenAI model\n",
    "            model = ChatOpenAI(openai_api_key=openai.api_key)\n",
    "            \n",
    "            # Define processing chain\n",
    "            chain = (\n",
    "                {\"context\": retriever, \"questions\": RunnablePassthrough()}\n",
    "                | prompt\n",
    "                | model\n",
    "                | StrOutputParser()\n",
    "            )\n",
    "            \n",
    "            # Invoke the processing chain with user input\n",
    "            response = chain.invoke(query)\n",
    "        \n",
    "        else:\n",
    "            print('The Data source format is not in required format')\n",
    "\n",
    "        return response\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:04<00:00,  4.32s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The document is about artist requirements for an event in Abu Dhabi by UltraTech Cement. It includes emails with artist options, costs, and availability for the event.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat = DataTalk(r'C:\\Users\\Samruddhi More\\Desktop\\LLM BE Template\\data\\document_0.docx')\n",
    "chat.generate_response('What is the document about?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import fitz  # PyMuPDF\n",
    "from langchain.document_loaders import CSVLoader, PyMuPDFLoader\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI\n",
    "from docx2pdf import convert\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import openai\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "load_dotenv()\n",
    "openai.api_key = os.environ.get(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataTalk:\n",
    "\n",
    "    def __init__(self, cdn_url):\n",
    "        self.cdn_url = cdn_url\n",
    "        self.filepath = self.download_file(cdn_url)\n",
    "\n",
    "    def download_file(self, url):\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            cdn_filename = url.split('/')[-1]\n",
    "            with open(cdn_filename, 'wb') as f:\n",
    "                f.write(response.content)\n",
    "            print(f\"File {cdn_filename} downloaded successfully.\")\n",
    "            return cdn_filename\n",
    "        else:\n",
    "            raise Exception(f\"Failed to download file from {url}\")\n",
    "\n",
    "    def source_type(self):\n",
    "        file = ''\n",
    "        folderpath = os.path.dirname(self.filepath)\n",
    "        filename = os.path.basename(self.filepath)\n",
    "        if filename.endswith('.xlsx'):\n",
    "            csvname = filename.split('.')[0] + '.csv'\n",
    "            file = os.path.join(folderpath, csvname)\n",
    "            df = pd.read_excel(self.filepath)\n",
    "            df.to_csv(file, index=False)\n",
    "        elif filename.endswith('.docx'):\n",
    "            pdfname = filename.split('.')[0] + '.pdf'\n",
    "            file = os.path.join(folderpath, pdfname)\n",
    "            convert(self.filepath, file)\n",
    "        else:\n",
    "            file = self.filepath\n",
    "\n",
    "        return file\n",
    "\n",
    "    def generate_response(self, query):\n",
    "        embeddings = OpenAIEmbeddings(openai_api_key=openai.api_key)\n",
    "        model_name = \"gpt-3.5-turbo-instruct\"\n",
    "        file = self.source_type()\n",
    "        response = ''\n",
    "\n",
    "        if file.endswith('.csv'):\n",
    "            loader = CSVLoader(file_path=file)\n",
    "            index_creator = VectorstoreIndexCreator(embedding=embeddings)\n",
    "            docsearch = index_creator.from_loaders([loader])\n",
    "            llm = OpenAI(model_name=model_name, openai_api_key=openai.api_key)\n",
    "            chain = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=docsearch.vectorstore.as_retriever(), input_key=\"question\")\n",
    "            response = chain({\"question\": query})\n",
    "            response = response['result']\n",
    "\n",
    "        elif file.lower().endswith('.pdf'):\n",
    "            pdf_reader = PdfReader(file)\n",
    "            text = \"\"\n",
    "            for page in pdf_reader.pages:\n",
    "                text += page.extract_text()\n",
    "\n",
    "            text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200, length_function=len)\n",
    "            chunks = text_splitter.split_text(text=text)\n",
    "\n",
    "            vector_store = FAISS.from_texts(chunks, embedding=embeddings)\n",
    "            retriever = vector_store.as_retriever()\n",
    "\n",
    "            template = \"\"\"Respond to the prompt based on the following context: {context}\n",
    "            Questions: {questions}\n",
    "            \"\"\"\n",
    "            prompt = ChatPromptTemplate.from_template(template)\n",
    "            model = ChatOpenAI(openai_api_key=openai.api_key)\n",
    "\n",
    "            chain = (\n",
    "                {\"context\": retriever, \"questions\": RunnablePassthrough()}\n",
    "                | prompt\n",
    "                | model\n",
    "                | StrOutputParser()\n",
    "            )\n",
    "\n",
    "            response = chain.invoke(query)\n",
    "\n",
    "        else:\n",
    "            print('The data source format is not in the required format.')\n",
    "\n",
    "        return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File minimal-document.pdf downloaded successfully.\n"
     ]
    },
    {
     "ename": "PdfReadError",
     "evalue": "EOF marker not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPdfReadError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m chat \u001b[38;5;241m=\u001b[39m DataTalk(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://github.com/py-pdf/sample-files/blob/main/001-trivial/minimal-document.pdf\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_response\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mWhat is the doc about?\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[7], line 52\u001b[0m, in \u001b[0;36mDataTalk.generate_response\u001b[1;34m(self, query)\u001b[0m\n\u001b[0;32m     49\u001b[0m     response \u001b[38;5;241m=\u001b[39m response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m file\u001b[38;5;241m.\u001b[39mlower()\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.pdf\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m---> 52\u001b[0m     pdf_reader \u001b[38;5;241m=\u001b[39m \u001b[43mPdfReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m page \u001b[38;5;129;01min\u001b[39;00m pdf_reader\u001b[38;5;241m.\u001b[39mpages:\n",
      "File \u001b[1;32mc:\\Users\\Samruddhi More\\Desktop\\LLM BE Template\\venv\\Lib\\site-packages\\PyPDF2\\_reader.py:319\u001b[0m, in \u001b[0;36mPdfReader.__init__\u001b[1;34m(self, stream, strict, password)\u001b[0m\n\u001b[0;32m    317\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(stream, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fh:\n\u001b[0;32m    318\u001b[0m         stream \u001b[38;5;241m=\u001b[39m BytesIO(fh\u001b[38;5;241m.\u001b[39mread())\n\u001b[1;32m--> 319\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m stream\n\u001b[0;32m    322\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_override_encryption \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Samruddhi More\\Desktop\\LLM BE Template\\venv\\Lib\\site-packages\\PyPDF2\\_reader.py:1415\u001b[0m, in \u001b[0;36mPdfReader.read\u001b[1;34m(self, stream)\u001b[0m\n\u001b[0;32m   1413\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread\u001b[39m(\u001b[38;5;28mself\u001b[39m, stream: StreamType) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1414\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_basic_validation(stream)\n\u001b[1;32m-> 1415\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_find_eof_marker\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1416\u001b[0m     startxref \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_find_startxref_pos(stream)\n\u001b[0;32m   1418\u001b[0m     \u001b[38;5;66;03m# check and eventually correct the startxref only in not strict\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Samruddhi More\\Desktop\\LLM BE Template\\venv\\Lib\\site-packages\\PyPDF2\\_reader.py:1471\u001b[0m, in \u001b[0;36mPdfReader._find_eof_marker\u001b[1;34m(self, stream)\u001b[0m\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m line[:\u001b[38;5;241m5\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%%\u001b[39;00m\u001b[38;5;124mEOF\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1470\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream\u001b[38;5;241m.\u001b[39mtell() \u001b[38;5;241m<\u001b[39m last_mb:\n\u001b[1;32m-> 1471\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m PdfReadError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEOF marker not found\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1472\u001b[0m     line \u001b[38;5;241m=\u001b[39m read_previous_line(stream)\n",
      "\u001b[1;31mPdfReadError\u001b[0m: EOF marker not found"
     ]
    }
   ],
   "source": [
    "chat = DataTalk('https://github.com/py-pdf/sample-files/blob/main/001-trivial/minimal-document.pdf')\n",
    "chat.generate_response('What is the doc about?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from langchain.document_loaders import CSVLoader\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "import PyPDF2\n",
    "from PyPDF2 import PdfReader\n",
    "from docx2pdf import convert\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import openai\n",
    "\n",
    "load_dotenv()\n",
    "openai.api_key = os.environ.get(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data source format is not in required format\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class DataTalk:\n",
    "\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "        self.filepath = self.download_file(url)\n",
    "\n",
    "    def download_file(self, url):\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            # Extract the filename from the URL and save it in the current directory\n",
    "            filename = url.split(\"/\")[-1].split(\"?\")[0]  # Remove query parameters if any\n",
    "            with open(filename, \"wb\") as file:\n",
    "                file.write(response.content)\n",
    "            return filename\n",
    "        else:\n",
    "            raise Exception(f\"Failed to download file from {url}\")\n",
    "\n",
    "    def source_type(self):\n",
    "        file = ''\n",
    "        folderpath = os.path.dirname(self.filepath)\n",
    "        filename = os.path.basename(self.filepath)\n",
    "        if filename.endswith('.xlsx'):\n",
    "            csvname = filename.split('.')[0] + '.csv'\n",
    "            file = os.path.join(folderpath, csvname)\n",
    "            df = pd.read_excel(self.filepath)\n",
    "            df.to_csv(file)\n",
    "        elif filename.endswith('.docx'):\n",
    "            pdfname = filename.split('.')[0] + '.pdf'\n",
    "            file = os.path.join(folderpath, pdfname)\n",
    "            convert(self.filepath, file)\n",
    "        else: \n",
    "            file = self.filepath\n",
    "\n",
    "        return file\n",
    "\n",
    "    def generate_response(self, query):\n",
    "        # Initialize the OpenAI embeddings\n",
    "        embeddings = OpenAIEmbeddings(openai_api_key=openai.api_key)\n",
    "        model_name = \"gpt-3.5-turbo-instruct\"\n",
    "        file = self.source_type()\n",
    "        response = ''\n",
    "\n",
    "        if file.endswith('.csv'):\n",
    "            # Load the documents\n",
    "            loader = CSVLoader(file_path=file)\n",
    "            # Create an index using the loaded documents\n",
    "            index_creator = VectorstoreIndexCreator(embedding=embeddings)\n",
    "            docsearch = index_creator.from_loaders([loader])\n",
    "            # Create a question-answering chain using the index\n",
    "            llm = OpenAI(model_name=model_name, openai_api_key=openai.api_key)\n",
    "            chain = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=docsearch.vectorstore.as_retriever(), input_key=\"question\")\n",
    "            response = chain({\"question\": query})\n",
    "            response = response['result']\n",
    "\n",
    "        elif file.lower().endswith('.pdf'):\n",
    "            \"\"\"\n",
    "            Process a PDF file, extracting text and splitting it into chunks.\n",
    "            \"\"\"\n",
    "            pdf_reader = PdfReader(file)\n",
    "            text = \"\"\n",
    "            for page in pdf_reader.pages:\n",
    "                text += page.extract_text()\n",
    "            \n",
    "            # Split text into chunks\n",
    "            text_splitter = RecursiveCharacterTextSplitter(\n",
    "                chunk_size=1000,\n",
    "                chunk_overlap=200,\n",
    "                length_function=len\n",
    "            )\n",
    "            chunks = text_splitter.split_text(text=text)\n",
    "\n",
    "            # Create vector store using FAISS\n",
    "            vector_store = FAISS.from_texts(chunks, embedding=embeddings)\n",
    "\n",
    "            \"\"\"\n",
    "            Generate responses based on prompts and embeddings.\n",
    "            \"\"\"\n",
    "            retriever = vector_store.as_retriever()\n",
    "            \n",
    "            # Define template for prompts\n",
    "            template = \"\"\"Respond to the prompt based on the following context: {context}\n",
    "            Questions: {questions}\n",
    "            \"\"\"\n",
    "            prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "            # Initialize ChatOpenAI model\n",
    "            model = ChatOpenAI(openai_api_key=openai.api_key)\n",
    "            \n",
    "            # Define processing chain\n",
    "            chain = (\n",
    "                {\"context\": retriever, \"questions\": RunnablePassthrough()}\n",
    "                | prompt\n",
    "                | model\n",
    "                | StrOutputParser()\n",
    "            )\n",
    "            \n",
    "            # Invoke the processing chain with user input\n",
    "            response = chain.invoke(query)\n",
    "        \n",
    "        else:\n",
    "            print('The data source format is not in required format')\n",
    "\n",
    "        return response\n",
    "\n",
    "# Example usage\n",
    "chat = DataTalk('https://drive.google.com/uc?id=1zO8ekHWx9U7mrbx_0Hoxxu6od7uxJqWw&export=download')\n",
    "response = chat.generate_response('What is the doc about?')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
